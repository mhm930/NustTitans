{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":["DObkW3ulM7yg"],"provenance":[],"gpuType":"T4"},"vscode":{"interpreter":{"hash":"d203a7fbe37afbb990fedfc21c321928443618f3d7b991e0237ff71906aa031f"}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9543481,"sourceType":"datasetVersion","datasetId":5813932},{"sourceId":10422993,"sourceType":"datasetVersion","datasetId":6460340},{"sourceId":10558270,"sourceType":"datasetVersion","datasetId":6532184},{"sourceId":10560824,"sourceType":"datasetVersion","datasetId":6534015}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##b. Clone or update competition repository\n\nAfter cloning, under MyDrive, you will see afrisenti-semeval-2023 folder with all the the data for the afrisenti shared task (training and dev)","metadata":{"id":"U-OZxUWIMqtq"}},{"cell_type":"markdown","source":"# Installation of Librararies\n","metadata":{}},{"cell_type":"code","source":"!pip install  transformers==4.45.2\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:21:35.800334Z","iopub.execute_input":"2025-02-08T18:21:35.800568Z","iopub.status.idle":"2025-02-08T18:22:04.585648Z","shell.execute_reply.started":"2025-02-08T18:21:35.800543Z","shell.execute_reply":"2025-02-08T18:22:04.584492Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.45.2\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (0.20.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.2) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2024.6.2)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed transformers-4.45.2\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"##c. Install required libraries\n\n- Set the project dire\nctory in the cell below, where the requirements file should also be located, and run the cell","metadata":{"id":"Xb03Gp9fUN8C"}},{"cell_type":"markdown","source":"# Training setup","metadata":{"id":"aDoyRlje3Rm7"}},{"cell_type":"markdown","source":"##a. Set project parameters\n\nFor a list of models that be used for fine-tuning, you can check [HERE](https://huggingface.co/models).","metadata":{"id":"8AaXec415s0f"}},{"cell_type":"code","source":"import os\nimport shutil\n\ndirectory_path = \"NustTitans\"\n\n# Check if the directory exists, delete it if it does, and then recreate it\nif os.path.exists(directory_path):\n    shutil.rmtree(directory_path)  # This deletes the directory and its contents\n\n# Now, create the directory again\nos.mkdir(\"NustTitans\")\nprint(f\"Directory '{directory_path}' has been recreated.\")\n\n# Create an empty file called \"empty_file.txt\"\nfile_name = \"NustTitans/run_textclass.py\"\n\n# Open the file in write mode (\"w\") — this creates the file if it doesn't exist\nwith open(file_name, \"w\") as file:\n    # Write some Python code to the file\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:22:20.598117Z","iopub.execute_input":"2025-02-08T18:22:20.598502Z","iopub.status.idle":"2025-02-08T18:22:20.605499Z","shell.execute_reply.started":"2025-02-08T18:22:20.598467Z","shell.execute_reply":"2025-02-08T18:22:20.604541Z"}},"outputs":[{"name":"stdout","text":"Directory 'NustTitans' has been recreated.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile /kaggle/working/NustTitans/adaptive_pretraining.py\n#!/usr/bin/env python\n# coding=utf-8\nimport transformers, torch\nimport logging\nimport os\nimport random\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n    set_seed,\n    AutoModelForMaskedLM,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.utils import check_min_version, send_example_telemetry\nfrom transformers.utils.versions import require_version\nfrom datasets import Features, Value, ClassLabel, Dataset\nfrom transformers import DataCollatorForLanguageModeling\n\n# Ensure required version of datasets library\nrequire_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass DataTrainingArguments:\n    max_seq_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated.\"\n        }\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n    )\n    pad_to_max_length: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to pad all samples to `max_seq_length`.\"}\n    )\n\n    domain_corpus: Optional[str] = field(\n        default=None, metadata={\"help\": \"Path to domain-specific corpus for adaptive pre-training\"}\n    )\n\n\n@dataclass\nclass ModelArguments:\n\n    model_name_or_path: str = field(default=None, metadata={\"help\": \"Path to pretrained model or model identifier\"})\n    config_name: Optional[str] = field(default=None, metadata={\"help\": \"Pretrained config name or path\"})\n    tokenizer_name: Optional[str] = field(default=None, metadata={\"help\": \"Pretrained tokenizer name or path\"})\n    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"Path to dataset\"})\n    cache_dir: Optional[str] = field(default=None, metadata={\"help\": \"Directory to store the pretrained models\"})\n    do_lower_case: Optional[bool] = field(default=False, metadata={\"help\": \"Lowercase tokenizer\"})\n    use_fast_tokenizer: bool = field(default=True, metadata={\"help\": \"Use fast tokenizer\"})\n    model_revision: str = field(default=\"main\", metadata={\"help\": \"Model version\"})\n    use_auth_token: bool = field(default=False, metadata={\"help\": \"Use auth token for private models\"})\n    ignore_mismatched_sizes: bool = field(default=False, metadata={\"help\": \"Enable loading model with mismatched head sizes\"})\n    epochs: Optional[int] = field(default=3, metadata={\"help\": \"Number of training epochs\"})\n    lr: Optional[float] = field(default=2e-5, metadata={\"help\": \"Learning rate for training\"})\n\n\ndef main():\n    # Parse arguments from command line\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n\n    logger.setLevel(training_args.get_process_log_level())\n    datasets.utils.logging.set_verbosity(training_args.get_process_log_level())\n    transformers.utils.logging.set_verbosity(training_args.get_process_log_level())\n\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n\n    # Handle last checkpoint detection\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint:\n            logger.info(f\"Checkpoint detected, resuming from {last_checkpoint}.\")\n        elif len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(f\"Output directory ({training_args.output_dir}) is not empty.\")\n\n    # Set seed for reproducibility\n    set_seed(training_args.seed)\n\n    num_labels = 6\n\n    # Model configuration\n    config = AutoConfig.from_pretrained(model_args.model_name_or_path, num_labels=num_labels)\n    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name or model_args.model_name_or_path, use_fast=model_args.use_fast_tokenizer)\n    model = AutoModelForMaskedLM.from_pretrained(model_args.model_name_or_path)\n\n    def preprocess_function(examples):\n        label_list = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n        texts = (examples['text'],)\n        result = tokenizer(*texts, padding=\"max_length\", max_length=data_args.max_seq_length, truncation=True)\n        if \"label\" in examples:\n            result[\"label\"] = [label_list.index(l) for l in examples[\"label\"]]\n        return result\n\n    if data_args.domain_corpus:\n        # Metric setup\n        metric = evaluate.load(\"accuracy\")\n\n        def compute_metrics(p: EvalPrediction):\n            preds = np.argmax(p.predictions, axis=1)\n            return metric.compute(predictions=preds, references=p.label_ids)\n\n        # Load your domain-specific corpus\n        domain_corpus = load_dataset(data_args.domain_corpus)  # Use the `domain_corpus` argument\n\n        # Tokenization for MLM\n        def preprocess_function(examples):\n            result = tokenizer(examples['text'], padding=True, truncation=True, max_length=data_args.max_seq_length)\n            return result\n\n        # Split domain_corpus into 'train' and 'test'\n        domain_corpus = domain_corpus.remove_columns(\"label\")\n        train_dataset = domain_corpus.map(preprocess_function, batched=True)[\"train\"]\n        eval_dataset =  domain_corpus.map(preprocess_function, batched=True)[\"test\"]\n\n        # Setup Data Collator for MLM\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=True,\n            mlm_probability=0.15,  # 15% of the tokens will be masked during training\n        )\n\n        # Fine-tuning the model on domain-specific corpus (Adaptive Pre-Training)\n        domain_train_args = TrainingArguments(\n            output_dir=training_args.output_dir,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=16,\n            num_train_epochs=model_args.epochs,  # Using command-line argument for epochs\n            save_steps=1,\n            logging_steps=1,\n            evaluation_strategy=\"steps\",\n            logging_dir=\"./logs\",\n            save_total_limit=1,\n            disable_tqdm=True,\n            report_to=\"none\",\n            save_strategy=\"no\",\n            learning_rate=model_args.lr,  # Using command-line argument for learning rate\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=domain_train_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n\n        # Start training\n        train_result = trainer.train()\n        print(\"Done adaptive pre-training!\")\n        metrics = train_result.metrics\n        trainer.save_model(training_args.output_dir)  # Saves the tokenizer too for easy upload\n\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:11:04.824657Z","iopub.execute_input":"2025-02-07T15:11:04.825077Z","iopub.status.idle":"2025-02-07T15:11:04.835126Z","shell.execute_reply.started":"2025-02-07T15:11:04.825042Z","shell.execute_reply":"2025-02-07T15:11:04.834178Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/NustTitans/adaptive_pretraining.py\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!python /kaggle/working/NustTitans/adaptive_pretraining.py \\\n--epochs 1 \\\n--domain_corpus  \"DGurgurov/sundanese_sa\"\\\n--model_name_or_path 'w11wo/sundanese-roberta-base' \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 8 \\\n--lr 5e-5 \\\n--num_train_epochs 10 \\\n--max_seq_length 128 \\\n--save_steps -10  \\\n--output_dir '/kaggle/working/domain_pretraining' \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:07:57.646728Z","iopub.execute_input":"2025-02-07T15:07:57.647431Z","iopub.status.idle":"2025-02-07T15:09:25.150545Z","shell.execute_reply.started":"2025-02-07T15:07:57.647397Z","shell.execute_reply":"2025-02-07T15:09:25.149637Z"}},"outputs":[{"name":"stdout","text":"Map: 100%|███████████████████████████| 381/381 [00:00<00:00, 8378.45 examples/s]\nMap: 100%|█████████████████████████████| 76/76 [00:00<00:00, 6788.78 examples/s]\nMap: 100%|██████████████████████████| 304/304 [00:00<00:00, 10098.91 examples/s]\nMap: 100%|██████████████████████████| 381/381 [00:00<00:00, 10770.79 examples/s]\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n{'loss': 4.0849, 'grad_norm': 23.796396255493164, 'learning_rate': 4.8958333333333335e-05, 'epoch': 0.020833333333333332}\n{'eval_loss': 3.764785051345825, 'eval_runtime': 1.379, 'eval_samples_per_second': 220.451, 'eval_steps_per_second': 13.778, 'epoch': 0.020833333333333332}\n{'loss': 3.2802, 'grad_norm': 18.949377059936523, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.041666666666666664}\n{'eval_loss': 3.7294881343841553, 'eval_runtime': 1.3644, 'eval_samples_per_second': 222.816, 'eval_steps_per_second': 13.926, 'epoch': 0.041666666666666664}\n{'loss': 4.0866, 'grad_norm': 26.004310607910156, 'learning_rate': 4.6875e-05, 'epoch': 0.0625}\n{'eval_loss': 3.894301652908325, 'eval_runtime': 1.3589, 'eval_samples_per_second': 223.705, 'eval_steps_per_second': 13.982, 'epoch': 0.0625}\n{'loss': 4.7793, 'grad_norm': 30.539522171020508, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.08333333333333333}\n{'eval_loss': 3.952956438064575, 'eval_runtime': 1.355, 'eval_samples_per_second': 224.35, 'eval_steps_per_second': 14.022, 'epoch': 0.08333333333333333}\n{'loss': 4.3927, 'grad_norm': 20.399009704589844, 'learning_rate': 4.4791666666666673e-05, 'epoch': 0.10416666666666667}\n{'eval_loss': 3.813934326171875, 'eval_runtime': 1.3557, 'eval_samples_per_second': 224.238, 'eval_steps_per_second': 14.015, 'epoch': 0.10416666666666667}\n{'loss': 3.1581, 'grad_norm': 24.330612182617188, 'learning_rate': 4.375e-05, 'epoch': 0.125}\n{'eval_loss': 3.886742115020752, 'eval_runtime': 1.3547, 'eval_samples_per_second': 224.405, 'eval_steps_per_second': 14.025, 'epoch': 0.125}\n{'loss': 4.8478, 'grad_norm': 15.91595458984375, 'learning_rate': 4.270833333333333e-05, 'epoch': 0.14583333333333334}\n{'eval_loss': 3.879004955291748, 'eval_runtime': 1.3764, 'eval_samples_per_second': 220.869, 'eval_steps_per_second': 13.804, 'epoch': 0.14583333333333334}\n{'loss': 3.5311, 'grad_norm': 24.594764709472656, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.16666666666666666}\n{'eval_loss': 3.70100998878479, 'eval_runtime': 1.3593, 'eval_samples_per_second': 223.652, 'eval_steps_per_second': 13.978, 'epoch': 0.16666666666666666}\n{'loss': 3.2789, 'grad_norm': 16.163307189941406, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.1875}\n{'eval_loss': 3.600466251373291, 'eval_runtime': 1.3639, 'eval_samples_per_second': 222.895, 'eval_steps_per_second': 13.931, 'epoch': 0.1875}\n{'loss': 3.0031, 'grad_norm': 14.870192527770996, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.20833333333333334}\n{'eval_loss': 3.70378041267395, 'eval_runtime': 1.3703, 'eval_samples_per_second': 221.847, 'eval_steps_per_second': 13.865, 'epoch': 0.20833333333333334}\n{'loss': 2.9728, 'grad_norm': 13.173259735107422, 'learning_rate': 3.854166666666667e-05, 'epoch': 0.22916666666666666}\n{'eval_loss': 3.7921812534332275, 'eval_runtime': 1.3567, 'eval_samples_per_second': 224.074, 'eval_steps_per_second': 14.005, 'epoch': 0.22916666666666666}\n{'loss': 4.2294, 'grad_norm': 18.71672248840332, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.25}\n{'eval_loss': 3.6889655590057373, 'eval_runtime': 1.3769, 'eval_samples_per_second': 220.792, 'eval_steps_per_second': 13.799, 'epoch': 0.25}\n{'loss': 3.5736, 'grad_norm': 17.64914894104004, 'learning_rate': 3.6458333333333336e-05, 'epoch': 0.2708333333333333}\n{'eval_loss': 3.628523111343384, 'eval_runtime': 1.3588, 'eval_samples_per_second': 223.727, 'eval_steps_per_second': 13.983, 'epoch': 0.2708333333333333}\n{'loss': 3.5936, 'grad_norm': 16.341047286987305, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.2916666666666667}\n{'eval_loss': 3.5705723762512207, 'eval_runtime': 1.354, 'eval_samples_per_second': 224.516, 'eval_steps_per_second': 14.032, 'epoch': 0.2916666666666667}\n{'loss': 2.8303, 'grad_norm': 19.4033145904541, 'learning_rate': 3.4375e-05, 'epoch': 0.3125}\n{'eval_loss': 3.625497817993164, 'eval_runtime': 1.3649, 'eval_samples_per_second': 222.731, 'eval_steps_per_second': 13.921, 'epoch': 0.3125}\n{'loss': 3.5879, 'grad_norm': 21.2325439453125, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.3333333333333333}\n{'eval_loss': 3.6077072620391846, 'eval_runtime': 1.355, 'eval_samples_per_second': 224.351, 'eval_steps_per_second': 14.022, 'epoch': 0.3333333333333333}\n{'loss': 5.0257, 'grad_norm': 20.451791763305664, 'learning_rate': 3.229166666666667e-05, 'epoch': 0.3541666666666667}\n{'eval_loss': 3.6172962188720703, 'eval_runtime': 1.3571, 'eval_samples_per_second': 224.015, 'eval_steps_per_second': 14.001, 'epoch': 0.3541666666666667}\n{'loss': 3.6124, 'grad_norm': 19.141990661621094, 'learning_rate': 3.125e-05, 'epoch': 0.375}\n{'eval_loss': 3.52860951423645, 'eval_runtime': 1.3597, 'eval_samples_per_second': 223.585, 'eval_steps_per_second': 13.974, 'epoch': 0.375}\n{'loss': 3.476, 'grad_norm': 16.899545669555664, 'learning_rate': 3.0208333333333334e-05, 'epoch': 0.3958333333333333}\n{'eval_loss': 3.5342442989349365, 'eval_runtime': 1.3593, 'eval_samples_per_second': 223.637, 'eval_steps_per_second': 13.977, 'epoch': 0.3958333333333333}\n{'loss': 3.7735, 'grad_norm': 16.62235450744629, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.4166666666666667}\n{'eval_loss': 3.6675832271575928, 'eval_runtime': 1.3593, 'eval_samples_per_second': 223.641, 'eval_steps_per_second': 13.978, 'epoch': 0.4166666666666667}\n{'loss': 3.2851, 'grad_norm': 15.757680892944336, 'learning_rate': 2.8125000000000003e-05, 'epoch': 0.4375}\n{'eval_loss': 3.7658443450927734, 'eval_runtime': 1.3611, 'eval_samples_per_second': 223.351, 'eval_steps_per_second': 13.959, 'epoch': 0.4375}\n{'loss': 3.3755, 'grad_norm': 18.466266632080078, 'learning_rate': 2.7083333333333332e-05, 'epoch': 0.4583333333333333}\n{'eval_loss': 3.45151686668396, 'eval_runtime': 1.3891, 'eval_samples_per_second': 218.849, 'eval_steps_per_second': 13.678, 'epoch': 0.4583333333333333}\n{'loss': 3.0285, 'grad_norm': 19.13644790649414, 'learning_rate': 2.604166666666667e-05, 'epoch': 0.4791666666666667}\n{'eval_loss': 3.608783483505249, 'eval_runtime': 1.359, 'eval_samples_per_second': 223.694, 'eval_steps_per_second': 13.981, 'epoch': 0.4791666666666667}\n{'loss': 4.5592, 'grad_norm': 20.846477508544922, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n{'eval_loss': 3.4464128017425537, 'eval_runtime': 1.3567, 'eval_samples_per_second': 224.07, 'eval_steps_per_second': 14.004, 'epoch': 0.5}\n{'loss': 3.5504, 'grad_norm': 16.00766944885254, 'learning_rate': 2.3958333333333334e-05, 'epoch': 0.5208333333333334}\n{'eval_loss': 3.6028225421905518, 'eval_runtime': 1.36, 'eval_samples_per_second': 223.528, 'eval_steps_per_second': 13.971, 'epoch': 0.5208333333333334}\n{'loss': 3.0078, 'grad_norm': 32.23997116088867, 'learning_rate': 2.2916666666666667e-05, 'epoch': 0.5416666666666666}\n{'eval_loss': 3.6119532585144043, 'eval_runtime': 1.3594, 'eval_samples_per_second': 223.635, 'eval_steps_per_second': 13.977, 'epoch': 0.5416666666666666}\n{'loss': 4.5903, 'grad_norm': 20.903583526611328, 'learning_rate': 2.1875e-05, 'epoch': 0.5625}\n{'eval_loss': 3.5995659828186035, 'eval_runtime': 1.3576, 'eval_samples_per_second': 223.923, 'eval_steps_per_second': 13.995, 'epoch': 0.5625}\n{'loss': 3.3948, 'grad_norm': 15.098947525024414, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.5833333333333334}\n{'eval_loss': 3.5684056282043457, 'eval_runtime': 1.3797, 'eval_samples_per_second': 220.339, 'eval_steps_per_second': 13.771, 'epoch': 0.5833333333333334}\n{'loss': 3.4995, 'grad_norm': 17.07719612121582, 'learning_rate': 1.9791666666666665e-05, 'epoch': 0.6041666666666666}\n{'eval_loss': 3.555898904800415, 'eval_runtime': 1.3844, 'eval_samples_per_second': 219.586, 'eval_steps_per_second': 13.724, 'epoch': 0.6041666666666666}\n{'loss': 3.184, 'grad_norm': 19.380434036254883, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.625}\n{'eval_loss': 3.452253580093384, 'eval_runtime': 1.3605, 'eval_samples_per_second': 223.452, 'eval_steps_per_second': 13.966, 'epoch': 0.625}\n{'loss': 4.8253, 'grad_norm': 18.85409164428711, 'learning_rate': 1.7708333333333335e-05, 'epoch': 0.6458333333333334}\n{'eval_loss': 3.495837926864624, 'eval_runtime': 1.3734, 'eval_samples_per_second': 221.344, 'eval_steps_per_second': 13.834, 'epoch': 0.6458333333333334}\n{'loss': 2.867, 'grad_norm': 17.263587951660156, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.6666666666666666}\n{'eval_loss': 3.6432178020477295, 'eval_runtime': 1.3577, 'eval_samples_per_second': 223.908, 'eval_steps_per_second': 13.994, 'epoch': 0.6666666666666666}\n{'loss': 4.0708, 'grad_norm': 23.863981246948242, 'learning_rate': 1.5625e-05, 'epoch': 0.6875}\n{'eval_loss': 3.3334696292877197, 'eval_runtime': 1.3712, 'eval_samples_per_second': 221.701, 'eval_steps_per_second': 13.856, 'epoch': 0.6875}\n{'loss': 2.7961, 'grad_norm': 17.145902633666992, 'learning_rate': 1.4583333333333335e-05, 'epoch': 0.7083333333333334}\n{'eval_loss': 3.5504672527313232, 'eval_runtime': 1.3601, 'eval_samples_per_second': 223.509, 'eval_steps_per_second': 13.969, 'epoch': 0.7083333333333334}\n{'loss': 2.9075, 'grad_norm': 19.372188568115234, 'learning_rate': 1.3541666666666666e-05, 'epoch': 0.7291666666666666}\n{'eval_loss': 3.454118013381958, 'eval_runtime': 1.3725, 'eval_samples_per_second': 221.497, 'eval_steps_per_second': 13.844, 'epoch': 0.7291666666666666}\n{'loss': 3.026, 'grad_norm': 18.319028854370117, 'learning_rate': 1.25e-05, 'epoch': 0.75}\n{'eval_loss': 3.412416934967041, 'eval_runtime': 1.3605, 'eval_samples_per_second': 223.443, 'eval_steps_per_second': 13.965, 'epoch': 0.75}\n{'loss': 3.7125, 'grad_norm': 19.11214256286621, 'learning_rate': 1.1458333333333333e-05, 'epoch': 0.7708333333333334}\n{'eval_loss': 3.5410313606262207, 'eval_runtime': 1.364, 'eval_samples_per_second': 222.867, 'eval_steps_per_second': 13.929, 'epoch': 0.7708333333333334}\n{'loss': 2.6039, 'grad_norm': 19.635353088378906, 'learning_rate': 1.0416666666666668e-05, 'epoch': 0.7916666666666666}\n{'eval_loss': 3.4069759845733643, 'eval_runtime': 1.3626, 'eval_samples_per_second': 223.102, 'eval_steps_per_second': 13.944, 'epoch': 0.7916666666666666}\n{'loss': 4.4687, 'grad_norm': 16.8935489654541, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.8125}\n{'eval_loss': 3.505078077316284, 'eval_runtime': 1.3609, 'eval_samples_per_second': 223.377, 'eval_steps_per_second': 13.961, 'epoch': 0.8125}\n{'loss': 3.0754, 'grad_norm': 15.272546768188477, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.8333333333333334}\n{'eval_loss': 3.537503719329834, 'eval_runtime': 1.3684, 'eval_samples_per_second': 222.165, 'eval_steps_per_second': 13.885, 'epoch': 0.8333333333333334}\n{'loss': 3.5754, 'grad_norm': 16.71778106689453, 'learning_rate': 7.2916666666666674e-06, 'epoch': 0.8541666666666666}\n{'eval_loss': 3.502079725265503, 'eval_runtime': 1.3639, 'eval_samples_per_second': 222.889, 'eval_steps_per_second': 13.931, 'epoch': 0.8541666666666666}\n{'loss': 3.5696, 'grad_norm': 18.015920639038086, 'learning_rate': 6.25e-06, 'epoch': 0.875}\n{'eval_loss': 3.4370269775390625, 'eval_runtime': 1.3734, 'eval_samples_per_second': 221.352, 'eval_steps_per_second': 13.834, 'epoch': 0.875}\n{'loss': 3.8429, 'grad_norm': 32.07794952392578, 'learning_rate': 5.208333333333334e-06, 'epoch': 0.8958333333333334}\n{'eval_loss': 3.4824576377868652, 'eval_runtime': 1.3878, 'eval_samples_per_second': 219.047, 'eval_steps_per_second': 13.69, 'epoch': 0.8958333333333334}\n{'loss': 3.8687, 'grad_norm': 14.880985260009766, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.9166666666666666}\n{'eval_loss': 3.3891425132751465, 'eval_runtime': 1.3607, 'eval_samples_per_second': 223.418, 'eval_steps_per_second': 13.964, 'epoch': 0.9166666666666666}\n{'loss': 2.5003, 'grad_norm': 14.481986999511719, 'learning_rate': 3.125e-06, 'epoch': 0.9375}\n{'eval_loss': 3.5528171062469482, 'eval_runtime': 1.3645, 'eval_samples_per_second': 222.788, 'eval_steps_per_second': 13.924, 'epoch': 0.9375}\n{'loss': 3.1756, 'grad_norm': 24.93450927734375, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.9583333333333334}\n{'eval_loss': 3.4550111293792725, 'eval_runtime': 1.364, 'eval_samples_per_second': 222.881, 'eval_steps_per_second': 13.93, 'epoch': 0.9583333333333334}\n{'loss': 3.7116, 'grad_norm': 20.006044387817383, 'learning_rate': 1.0416666666666667e-06, 'epoch': 0.9791666666666666}\n{'eval_loss': 3.500225782394409, 'eval_runtime': 1.3634, 'eval_samples_per_second': 222.979, 'eval_steps_per_second': 13.936, 'epoch': 0.9791666666666666}\n{'loss': 2.5933, 'grad_norm': 21.95258903503418, 'learning_rate': 0.0, 'epoch': 1.0}\n{'eval_loss': 3.480191707611084, 'eval_runtime': 1.376, 'eval_samples_per_second': 220.925, 'eval_steps_per_second': 13.808, 'epoch': 1.0}\n{'train_runtime': 72.852, 'train_samples_per_second': 5.23, 'train_steps_per_second': 0.659, 'train_loss': 3.578823044896126, 'epoch': 1.0}\nDone adaptive pre-training!\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"%%writefile /kaggle/working/NustTitans/fine_tuning.py\n#!/usr/bin/env python\n# coding=utf-8\nimport transformers,torch\nimport logging\nimport os\nimport random\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.utils import check_min_version, send_example_telemetry\nfrom transformers.utils.versions import require_version\nfrom datasets import Features, Value, ClassLabel, Dataset\n\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Ensure required version of datasets library\nrequire_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n\nlogger = logging.getLogger(__name__)\n\nimport re\nimport string\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess the input text by removing emojis, punctuation, extra spaces, etc.\n    \"\"\"\n\n    # Remove emojis using regex (Unicode characters for emojis)\n    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"\n                               \"\\U0001F300-\\U0001F5FF\"\n                               \"\\U0001F680-\\U0001F6FF\"\n                               \"\\U0001F700-\\U0001F77F\"\n                               \"\\U0001F780-\\U0001F7FF\"\n                               \"\\U0001F800-\\U0001F8FF\"\n                               \"\\U0001F900-\\U0001F9FF\"\n                               \"\\U0001FA00-\\U0001FA6F\"\n                               \"\\U0001FA70-\\U0001FAFF\"\n                               \"\\U00002702-\\U000027B0\"\n                               \"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = re.sub(emoji_pattern, '', text)  # Remove emojis\n\n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n\n    # Remove mentions (e.g., @username)\n    text = re.sub(r\"@\\w+\", '', text)\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Remove extra whitespace (e.g., multiple spaces, newlines)\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\n\n@dataclass\nclass DataTrainingArguments:\n    max_seq_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated.\"\n        }\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n    )\n    pad_to_max_length: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to pad all samples to `max_seq_length`.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=None, metadata={\"help\": \"For debugging purposes, truncate the number of training examples.\"}\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None, metadata={\"help\": \"For debugging purposes, truncate the number of evaluation examples.\"}\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None, metadata={\"help\": \"For debugging purposes, truncate the number of prediction examples.\"}\n    )\n@dataclass\nclass ModelArguments:\n\n    model_name_or_path: str = field(default=None, metadata={\"help\": \"Path to pretrained model or model identifier\"})\n    config_name: Optional[str] = field(default=None, metadata={\"help\": \"Pretrained config name or path\"})\n    tokenizer_name: Optional[str] = field(default=None, metadata={\"help\": \"Pretrained tokenizer name or path\"})\n    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"Path to dataset\"})\n    cache_dir: Optional[str] = field(default=None, metadata={\"help\": \"Directory to store the finetuned models\"})\n    do_lower_case: Optional[bool] = field(default=False, metadata={\"help\": \"Lowercase tokenizer\"})\n    use_fast_tokenizer: bool = field(default=True, metadata={\"help\": \"Use fast tokenizer\"})\n    model_revision: str = field(default=\"main\", metadata={\"help\": \"Model version\"})\n    use_auth_token: bool = field(default=False, metadata={\"help\": \"Use auth token for private models\"})\n    ignore_mismatched_sizes: bool = field(default=False, metadata={\"help\": \"Enable loading model with mismatched head sizes\"})\n    lr: Optional[float] = field(default=2e-5, metadata={\"help\": \"Learning rate for training\"})\n    epochs: Optional[int] = field(default=3, metadata={\"help\": \"Number of training epochs\"})\ndef main():\n    # Parse arguments from command line\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        handlers=[logging.StreamHandler(sys.stdout)],\n    )\n\n    logger.setLevel(training_args.get_process_log_level())\n    datasets.utils.logging.set_verbosity(training_args.get_process_log_level())\n    transformers.utils.logging.set_verbosity(training_args.get_process_log_level())\n\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n\n    # Handle last checkpoint detection\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint:\n            logger.info(f\"Checkpoint detected, resuming from {last_checkpoint}.\")\n        elif len(os.listdir(training_args.output_dir)) > 0:\n            raise ValueError(f\"Output directory ({training_args.output_dir}) is not empty.\")\n\n    # Set seed for reproducibility\n    set_seed(training_args.seed)\n    num_labels = 6\n    config = AutoConfig.from_pretrained(model_args.model_name_or_path, num_labels=num_labels)\n    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name or model_args.model_name_or_path, use_fast=model_args.use_fast_tokenizer)\n    # Load the fine-tuning dataset (for example, a sentiment analysis dataset)\n    df = pd.read_csv(model_args.data_dir)  # Replace with your fine-tuning data\n    # Convert labels to float32 for multi-label classification\n    df['labels'] = df[['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']].astype('float32').values.tolist()\n # Multi-label column\n    df['text'] = df['text'].apply(preprocess_text)  # Apply text preprocessing\n\n    # Convert dataset into Hugging Face Dataset format\n    from datasets import Dataset\n    fine_tuning_dataset = Dataset.from_pandas(df)\n    # Ensure labels are of float32 type\n    fine_tuning_dataset = fine_tuning_dataset.map(\n        lambda x: {'labels': torch.tensor(x['labels'], dtype=torch.float32)},\n        batched=True\n    )\n\n    # Preprocessing function for fine-tuning (same tokenizer)\n    def fine_tune_preprocess(examples):\n        return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n\n    # Tokenize the fine-tuning dataset\n    fine_tuning_dataset = fine_tuning_dataset.map(fine_tune_preprocess, batched=True)\n\n    # Convert to PyTorch format (input_ids, attention_mask, labels)\n    #fine_tuning_dataset = fine_tuning_dataset.rename_column(\"label\", \"labels\")\n    fine_tuning_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    data_collator = DataCollatorWithPadding(tokenizer)\n    # Load the model for fine-tuning\n    model_for_fine_tuning = AutoModelForSequenceClassification.from_pretrained(\n        num_labels=6,  # For multi-label classification (number of labels)\n        problem_type=\"multi_label_classification\",  # Multi-label classification setup\n        pretrained_model_name_or_path = model_args.model_name_or_path,\n\n    )\n\n    # Fine-tuning training arguments\n    fine_tuning_args = TrainingArguments(\n        output_dir=training_args.output_dir,\n        evaluation_strategy=\"epoch\",\n        learning_rate=model_args.lr,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=model_args.epochs,\n        weight_decay=0.01,\n        save_strategy=\"no\",\n\n        \n    )\n\n    # Set up Trainer for fine-tuning\n    trainer_fine_tune = Trainer(\n        model=model_for_fine_tuning,\n        args=fine_tuning_args,\n        train_dataset=fine_tuning_dataset,\n        eval_dataset=fine_tuning_dataset,  # You can use separate validation set\n        data_collator = data_collator,\n    )\n\n    # Start fine-tuning the model\n    trainer_results = trainer_fine_tune.train()\n    metrics = trainer_results.metrics\n    max_train_samples = (data_args.max_train_samples if data_args.max_train_samples is not None else len(fine_tuning_dataset))\n    metrics[\"train_samples\"] = min(max_train_samples, len(fine_tuning_dataset))\n\n    trainer_fine_tune.save_model(training_args.output_dir)  # Saves the tokenizer too for easy upload\n    tokenizer.save_pretrained(training_args.output_dir)\n    trainer_fine_tune.log_metrics(\"train\", metrics)\n    trainer_fine_tune.save_metrics(\"train\", metrics)\n    trainer_fine_tune.save_state()\n\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:22:37.636547Z","iopub.execute_input":"2025-02-08T18:22:37.637001Z","iopub.status.idle":"2025-02-08T18:22:37.646143Z","shell.execute_reply.started":"2025-02-08T18:22:37.636966Z","shell.execute_reply":"2025-02-08T18:22:37.645224Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/NustTitans/fine_tuning.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#Fine_tuning\n!python /kaggle/working/NustTitans/fine_tuning.py \\\n  --model_name_or_path '/kaggle/working/domain_pretraining'  \\\n  --output_dir '/kaggle/working/finetuning_3' \\\n  --data_dir '/kaggle/input/final-dataset-semeval2025/public_data_test/track_a/train/sun.csv' \\\n  --do_train \\\n  --per_device_train_batch_size 8 \\\n  --lr 5e-5 \\\n  --epochs 1 \\\n  --max_seq_length 128 \\\n  --save_steps -10                                                       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:28:02.889400Z","iopub.execute_input":"2025-02-07T15:28:02.889744Z","iopub.status.idle":"2025-02-07T15:28:29.573782Z","shell.execute_reply.started":"2025-02-07T15:28:02.889713Z","shell.execute_reply":"2025-02-07T15:28:29.572827Z"}},"outputs":[{"name":"stdout","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nMap: 100%|██████████████████████████| 924/924 [00:00<00:00, 62802.41 examples/s]\nMap: 100%|██████████████████████████| 924/924 [00:00<00:00, 13530.39 examples/s]\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/domain_pretraining and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n 99%|████████████████████████████████████████▋| 115/116 [00:11<00:00,  9.81it/s]\n  0%|                                                   | 0/116 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                         | 5/116 [00:00<00:02, 49.89it/s]\u001b[A\n  9%|███▌                                      | 10/116 [00:00<00:02, 43.65it/s]\u001b[A\n 13%|█████▍                                    | 15/116 [00:00<00:02, 42.02it/s]\u001b[A\n 17%|███████▏                                  | 20/116 [00:00<00:02, 41.29it/s]\u001b[A\n 22%|█████████                                 | 25/116 [00:00<00:02, 40.95it/s]\u001b[A\n 26%|██████████▊                               | 30/116 [00:00<00:02, 40.62it/s]\u001b[A\n 30%|████████████▋                             | 35/116 [00:00<00:02, 40.29it/s]\u001b[A\n 34%|██████████████▍                           | 40/116 [00:00<00:01, 40.28it/s]\u001b[A\n 39%|████████████████▎                         | 45/116 [00:01<00:01, 40.31it/s]\u001b[A\n 43%|██████████████████                        | 50/116 [00:01<00:01, 40.28it/s]\u001b[A\n 47%|███████████████████▉                      | 55/116 [00:01<00:01, 40.33it/s]\u001b[A\n 52%|█████████████████████▋                    | 60/116 [00:01<00:01, 40.18it/s]\u001b[A\n 56%|███████████████████████▌                  | 65/116 [00:01<00:01, 40.07it/s]\u001b[A\n 60%|█████████████████████████▎                | 70/116 [00:01<00:01, 40.05it/s]\u001b[A\n 65%|███████████████████████████▏              | 75/116 [00:01<00:01, 39.91it/s]\u001b[A\n 68%|████████████████████████████▌             | 79/116 [00:01<00:00, 39.75it/s]\u001b[A\n 72%|██████████████████████████████            | 83/116 [00:02<00:00, 39.68it/s]\u001b[A\n 75%|███████████████████████████████▌          | 87/116 [00:02<00:00, 39.57it/s]\u001b[A\n 79%|█████████████████████████████████▎        | 92/116 [00:02<00:00, 39.69it/s]\u001b[A\n 83%|██████████████████████████████████▊       | 96/116 [00:02<00:00, 39.59it/s]\u001b[A\n 87%|███████████████████████████████████▋     | 101/116 [00:02<00:00, 39.75it/s]\u001b[A\n 91%|█████████████████████████████████████▍   | 106/116 [00:02<00:00, 39.92it/s]\u001b[A\n 96%|███████████████████████████████████████▏ | 111/116 [00:02<00:00, 40.06it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3730302155017853, 'eval_runtime': 2.9045, 'eval_samples_per_second': 318.132, 'eval_steps_per_second': 39.939, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 116/116 [00:14<00:00,  9.81it/s]\n100%|█████████████████████████████████████████| 116/116 [00:02<00:00, 40.31it/s]\u001b[A\n{'train_runtime': 14.9282, 'train_samples_per_second': 61.896, 'train_steps_per_second': 7.771, 'train_loss': 0.4121156560963598, 'epoch': 1.0}\n100%|█████████████████████████████████████████| 116/116 [00:14<00:00,  7.77it/s]\n***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =    38032GF\n  train_loss               =     0.4121\n  train_runtime            = 0:00:14.92\n  train_samples            =        924\n  train_samples_per_second =     61.896\n  train_steps_per_second   =      7.771\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import re\nimport string\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess the input text by removing emojis, punctuation, extra spaces, etc.\n    \"\"\"\n    \n    # Remove emojis using regex (Unicode characters for emojis)\n    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"\n                               \"\\U0001F300-\\U0001F5FF\"\n                               \"\\U0001F680-\\U0001F6FF\"\n                               \"\\U0001F700-\\U0001F77F\"\n                               \"\\U0001F780-\\U0001F7FF\"\n                               \"\\U0001F800-\\U0001F8FF\"\n                               \"\\U0001F900-\\U0001F9FF\"\n                               \"\\U0001FA00-\\U0001FA6F\"\n                               \"\\U0001FA70-\\U0001FAFF\"\n                               \"\\U00002702-\\U000027B0\"\n                               \"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = re.sub(emoji_pattern, '', text)  # Remove emojis\n    \n    # Remove URLs\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n    e98-0\n    # Remove mentions (e.g., @username)\n    text = re.sub(r\"@\\w+\", '', text)\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', ''F, string.punctuation))\n    \n    # Remove extra whitespace (e.g., multiple spaces, newlines)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:22:45.544964Z","iopub.execute_input":"2025-02-08T18:22:45.545306Z","iopub.status.idle":"2025-02-08T18:22:45.553090Z","shell.execute_reply.started":"2025-02-08T18:22:45.545277Z","shell.execute_reply":"2025-02-08T18:22:45.551794Z"}},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[5], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    text = text.translate(str.maketrans('', ''F, string.punctuation))\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"],"ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (2748692732.py, line 30)","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"As you may observe, the training loss is very large. As a start, you can tune the training parameters and model to get a competitive result.\n\nYou can observe also, there is no validation metrics (e.g., accuracy, loss etc) since we are only training without validtaion","metadata":{"id":"wZtt5lBBLPz1"}},{"cell_type":"markdown","source":"# Prediction on Eval dataset\n\n1. F1-score (macro)\n2. F1-score (micro)\n\nEvaluation based in emotions seperetaley","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport torch\nfrom datasets import Dataset\nimport pandas as pd\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/finetuning_3/\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/finetuning_3/\")\n\n# If you're predicting on a test set, load it (example below)\ntest_data = pd.read_csv('/kaggle/input/final-dataset-semeval2025/public_data_test/track_a/dev/sun.csv')  # Replace with your test data path\ntest_data['text'] = test_data['text'].apply(preprocess_text)  # Apply the same preprocessing as for training\n\n# Convert to Hugging Face Dataset format\ntest_dataset = Dataset.from_pandas(test_data)\n\n# Tokenize the test dataset\ndef predict_preprocess(examples):\n    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n\ntest_dataset = test_dataset.map(predict_preprocess, batched=True)\n\n# Set format for PyTorch\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# Define Trainer for prediction (no need for a training loop)\ntrainer = Trainer(\n    model=model,  # The fine-tuned model\n    args=TrainingArguments(output_dir='/kaggle/working/finetuning_3', do_train=False, do_eval=False),  # Empty args for prediction\n    tokenizer=tokenizer\n)\n\n# Make predictions\npredictions, labels, _ = trainer.predict(test_dataset)\n\n# Get the predicted labels (for multi-label classification, we can threshold predictions)\npredicted_labels = (torch.sigmoid(torch.tensor(predictions)) > .3).int()  # For multi-label, use sigmoid activation and threshold\n\n# Print out or process the predictions\nprint(predicted_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:30:53.569190Z","iopub.execute_input":"2025-02-07T15:30:53.569872Z","iopub.status.idle":"2025-02-07T15:30:54.462696Z","shell.execute_reply.started":"2025-02-07T15:30:53.569838Z","shell.execute_reply":"2025-02-07T15:30:54.461887Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/199 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f874dfc2a044b2f80bc1d03f1c11e2e"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"tensor([[0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 1, 0],\n        ...,\n        [0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0, 0]], dtype=torch.int32)\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n# 11. Get the true labels for all classes and convert to 0 or 1\ntrue_labels = test_data[[\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]].fillna(0).applymap(lambda x: 1 if x > 0 else 0)\nprint(len(true_labels),len(predicted_labels))\n# 12. Calculate the F1 score for each class\nf1_scores = {}\nfor idx, label in enumerate(true_labels.columns):\n    f1 = f1_score(true_labels[label], predicted_labels[:, idx].numpy(), average='binary')\n    f1_scores[label] = f1\n\n# 13. Print F1 scores for each label\nfor label, score in f1_scores.items():\n    print(f\"F1 Score for {label}: {score}\")\n\n# Micro-average F1 score: Flatten both true labels and predicted labels\nmicro_f1 = f1_score(true_labels.values.flatten(), predicted_labels.flatten(), average='micro')\n\n# Macro-average F1 score: F1 score per label, then take the average\nmacro_f1 = f1_score(true_labels.values, predicted_labels, average='micro')\n\n# 13. Print out the results\nprint(f\"Micro-average F1 Score: {micro_f1}\")\nprint(f\"Macro-average F1 Score: {macro_f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:31:16.930593Z","iopub.execute_input":"2025-02-07T15:31:16.931452Z","iopub.status.idle":"2025-02-07T15:31:16.962376Z","shell.execute_reply.started":"2025-02-07T15:31:16.931415Z","shell.execute_reply":"2025-02-07T15:31:16.961497Z"}},"outputs":[{"name":"stdout","text":"199 199\nF1 Score for anger: 0.0\nF1 Score for disgust: 0.0\nF1 Score for fear: 0.0\nF1 Score for joy: 0.8430232558139535\nF1 Score for sadness: 0.5365853658536586\nF1 Score for surprise: 0.039999999999999994\nMicro-average F1 Score: 0.8308207705192628\nMacro-average F1 Score: 0.6392857142857143\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/829936282.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  true_labels = test_data[[\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]].fillna(0).applymap(lambda x: 1 if x > 0 else 0)\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"# Prediction on test Dataset","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport torch\nfrom datasets import Dataset\nos.environ['WANDB_DISABLED'] = 'true'\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/finetuning_3\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/finetuning_3\")\n\n# If you're predicting on a test set, load it (example below)\ntest_data = pd.read_csv('/kaggle/input/final-dataset-semeval2025/public_data_test/track_a/test/sun.csv')  # Replace with your test data path\ntest_data['text'] = test_data['text'].apply(preprocess_text)  # Apply the same preprocessing as for training\n\n# Convert to Hugging Face Dataset format\ntest_dataset = Dataset.from_pandas(test_data)\n\n# Tokenize the test dataset\ndef predict_preprocess(examples):\n    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)\n\ntest_dataset = test_dataset.map(predict_preprocess, batched=True)\n\n# Set format for PyTorch\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# Define Trainer for prediction (no need for a training loop)\ntrainer = Trainer(\n    model=model,  # The fine-tuned model\n    args=TrainingArguments(output_dir='/tmp', do_train=False, do_eval=False),  # Empty args for prediction\n    tokenizer=tokenizer\n)\n\n# Make predictions\npredictions, labels, _ = trainer.predict(test_dataset)\n\n# Get the predicted labels (for multi-label classification, we can threshold predictions)\npredicted_labels = (torch.sigmoid(torch.tensor(predictions)) > 0.4).int()  # For multi-label, use sigmoid activation and threshold\n\n# Print out or process the predictions\nprint(predicted_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:38:31.962495Z","iopub.execute_input":"2025-02-07T15:38:31.963259Z","iopub.status.idle":"2025-02-07T15:38:34.622481Z","shell.execute_reply.started":"2025-02-07T15:38:31.963222Z","shell.execute_reply":"2025-02-07T15:38:34.621741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/926 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c9406c4312f47bcba4fa64eaddbd59c"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"tensor([[0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 1, 0],\n        [0, 0, 0, 1, 1, 0],\n        ...,\n        [0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 1, 0],\n        [0, 0, 0, 1, 1, 0]], dtype=torch.int32)\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"# 1. Convert predicted labels to a DataFrame\npredicted_labels_df = pd.DataFrame(predicted_labels.numpy(), columns=[\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"])\n\n# 2. Create a DataFrame that includes the original IDs and the predicted labels\noutput_df = test_data[['id']].copy()  # Assuming 'id' column exists in the test data\noutput_df = output_df.join(predicted_labels_df)\n\n# 3. Save the DataFrame to a CSV file\noutput_df.to_csv('/kaggle/working/pred_sun.csv', index=False)\n\nprint(\"Predicted labels have been saved to predicted_labels.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T15:38:51.114718Z","iopub.execute_input":"2025-02-07T15:38:51.115570Z","iopub.status.idle":"2025-02-07T15:38:51.131022Z","shell.execute_reply.started":"2025-02-07T15:38:51.115530Z","shell.execute_reply":"2025-02-07T15:38:51.130117Z"}},"outputs":[{"name":"stdout","text":"Predicted labels have been saved to predicted_labels.csv\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"# Push Code to Github","metadata":{}},{"cell_type":"code","source":"!apt-get install git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:23:04.262119Z","iopub.execute_input":"2025-02-08T18:23:04.262827Z","iopub.status.idle":"2025-02-08T18:23:07.294786Z","shell.execute_reply.started":"2025-02-08T18:23:04.262793Z","shell.execute_reply":"2025-02-08T18:23:07.293924Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit is already the newest version (1:2.34.1-1ubuntu1.11).\n0 upgraded, 0 newly installed, 0 to remove and 72 not upgraded.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!git config --global user.name \"mhm930\"\n!git config --global user.email \"mhmkhan80@gmail.com\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:23:37.773175Z","iopub.execute_input":"2025-02-08T18:23:37.773603Z","iopub.status.idle":"2025-02-08T18:23:39.835028Z","shell.execute_reply.started":"2025-02-08T18:23:37.773566Z","shell.execute_reply":"2025-02-08T18:23:39.833826Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!git init\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:23:43.253952Z","iopub.execute_input":"2025-02-08T18:23:43.254323Z","iopub.status.idle":"2025-02-08T18:23:44.332392Z","shell.execute_reply.started":"2025-02-08T18:23:43.254290Z","shell.execute_reply":"2025-02-08T18:23:44.331326Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit branch -m <name>\u001b[m\nInitialized empty Git repository in /kaggle/working/.git/\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!git remote add origin https://github.com/mhm930/NustTitans.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:23:48.232234Z","iopub.execute_input":"2025-02-08T18:23:48.232613Z","iopub.status.idle":"2025-02-08T18:23:49.234761Z","shell.execute_reply.started":"2025-02-08T18:23:48.232577Z","shell.execute_reply":"2025-02-08T18:23:49.233809Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!git add .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:23:52.297456Z","iopub.execute_input":"2025-02-08T18:23:52.297840Z","iopub.status.idle":"2025-02-08T18:23:53.284419Z","shell.execute_reply.started":"2025-02-08T18:23:52.297807Z","shell.execute_reply":"2025-02-08T18:23:53.283416Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!git commit -m \"Add my code from Kaggle\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:23:55.952962Z","iopub.execute_input":"2025-02-08T18:23:55.953682Z","iopub.status.idle":"2025-02-08T18:23:57.013166Z","shell.execute_reply.started":"2025-02-08T18:23:55.953649Z","shell.execute_reply":"2025-02-08T18:23:57.012096Z"}},"outputs":[{"name":"stdout","text":"[master (root-commit) 7552cf2] Add my code from Kaggle\n 2 files changed, 221 insertions(+)\n create mode 100644 NustTitans/fine_tuning.py\n create mode 100644 NustTitans/run_textclass.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!git config --global user.name \"mhm930\"\n!git config --global user.password \"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\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:24:01.168513Z","iopub.execute_input":"2025-02-08T18:24:01.168923Z","iopub.status.idle":"2025-02-08T18:24:03.142270Z","shell.execute_reply.started":"2025-02-08T18:24:01.168887Z","shell.execute_reply":"2025-02-08T18:24:03.141263Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!git push origin master mhm930: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\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:30:15.182703Z","iopub.execute_input":"2025-02-08T18:30:15.183687Z","iopub.status.idle":"2025-02-08T18:30:16.185882Z","shell.execute_reply.started":"2025-02-08T18:30:15.183650Z","shell.execute_reply":"2025-02-08T18:30:16.184811Z"}},"outputs":[{"name":"stdout","text":"error: src refspec mhm930 does not match any\n\u001b[31merror: failed to push some refs to 'https://github.com/mhm930/NustTitans.git'\n\u001b[m","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"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","metadata":{}},{"cell_type":"code","source":"!git push https://mhm930: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@github.com/mhm930/NustTitans.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T18:24:16.706472Z","iopub.execute_input":"2025-02-08T18:24:16.707250Z","iopub.status.idle":"2025-02-08T18:24:18.381155Z","shell.execute_reply.started":"2025-02-08T18:24:16.707213Z","shell.execute_reply":"2025-02-08T18:24:18.379961Z"}},"outputs":[{"name":"stdout","text":"Enumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (5/5), 3.22 KiB | 3.22 MiB/s, done.\nTotal 5 (delta 0), reused 0 (delta 0), pack-reused 0\nremote: \nremote: Create a pull request for 'master' on GitHub by visiting:\u001b[K\nremote:      https://github.com/mhm930/NustTitans/pull/new/master\u001b[K\nremote: \nTo https://github.com/mhm930/NustTitans.git\n * [new branch]      master -> master\n","output_type":"stream"}],"execution_count":14}]}